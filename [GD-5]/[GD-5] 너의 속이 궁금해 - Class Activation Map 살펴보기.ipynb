{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stable-pastor",
   "metadata": {},
   "source": [
    "# 5-1. 들어가며\n",
    "\n",
    "지금까지 딥러닝 모델을 기반으로 이미지를 분류, 탐지하는 여러 가지 모델을 공부해 오셨을 것입니다. 이미지를 모델의 입력에 맞게 전처리해주고 모델에 통과시킨 뒤 결과를 해석해서 추론값을 얻어내는 과정이 익숙하실 것입니다.\n",
    "\n",
    "그런데 모델이 잘 동작한다는 것을 확인하고 끝내기에는 궁금한 것들이 많습니다. 예를 들면 개와 고양이를 분류하는 딥러닝 모델을 만들었다고 합시다. 이 모델이 이미지의 어느 부분을 보고 개라는 결론을 내렸는지 알 수 있을까요? 그동안 우리가 다룬 딥러닝은 모델의 추론 근거를 알 수 없는 블랙박스(Black Box) 모델이었습니다. 그래서 이 모델을 신뢰할 수 있는지조차 명확하지 않았죠. 그래서 이번 시간에는 모델과 추론의 신뢰성에 대한 답을 찾는 XAI(Explainable Artificial Intelligence, 설명 가능한 인공지능) 분야에 대해 알아보겠습니다. XAI 기법은 모델의 성능을 개선할 수 있는 단서로도 유용하게 활용될 것입니다.\n",
    "\n",
    "### 실습목표\n",
    "---\n",
    "1. 분류 모델의 활성화 맵을 이해합니다.\n",
    "2. 다양한 활성화 맵을 구하는 방법을 알아갑니다.\n",
    "3. 약지도학습(weakly supervised learning)을 이해합니다.\n",
    "\n",
    "### 학습 내용\n",
    "---\n",
    "1. Explainable AI\n",
    "2. CAM: Class Activation Map\n",
    "3. Grad-CAM\n",
    "4. ACoL: Adversarial Complementary Learning\n",
    "5. 생각해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
